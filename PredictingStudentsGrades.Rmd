---
title: Predicting Studentsâ€™ Final Grades Using Machine Learning Methods with Online
  Course Data
author: "Dustin"
date: "3/23/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
I will be following a walkthrough that applies machine learning to education. My goal is to learn more about machine learning in R and how to use it in education. The walkthrough will be from an open-access textbook about [Data Science in Education](https://datascienceineducation.com/c14.html). 

From the dataset I am working with, I will predict a final grade from variables within the dataset. I am not worried about how the variables relate to the outcome I am trying to predict for this task. I will be using random forest modeling as my method to predict the final grade outcome. 

## Background on the Dataset
Online learning has become a popular way to learn and to educate people. When people use online platforms, they submit quizzes, submit homework, and spend time learning in a learning management system. This process is similar to in-person teaching to facilitate learning. However, in-person instruction can rely on students' behavioral cues to help gauge whether the student is engaged in the learning environment. When measuring students ' engagement in an online course, it is more difficult for instructors to use specific behavioral cues in an online learning environment. Usually, these cues are in the form of missing class repeatedly or being distracted, but this is more difficult to measure in an online learning situation.

Although online learning lacks some of the behavioral cues from in-person instruction, instructors can collect different types of data in a learning management system to gauge student engagement. This other way is by tracking student interactions with the learning management system. For example, when a student watches a posted video, the system collects data about how long the student watches the video before pausing or logging out. Therefore, educators can find ways to support students in online environments from the data being collected like they can for in-person instruction.  

I will be examining a dataset that collected data on students' educational experiences attending an online science course at a virtual middle school. I want to characterize the students' motivation to achieve and their tangible engagement with the course. The dataset has self-reported motivation data and behavioral data from the learning management system that I will use to predict a final course grade. 

## Research Questions
I will explore the following four questions:
1. Is motivation more predictive of course grades than other online engagement indicators?
2. Which types of motivation are most predictive of achievement?
3. Which types of trace measures are most predictive of achievement?
4. How does a random forest compare to a simple linear model (regression)?

## Data Sources
The dataset has 499 students enrolled in an online middle school science course in 2015-2016.

Specific information in the dataset includes:
1. A self-report survey assessing three aspects of students' motivation
2. Log-trace data, such as data output from the learning management system
3. Discussion board data
4. Academic achievement data

I want to fit the simplest possible model to the data. The effectiveness in predicting an outcome is the most important thing, not the fanciness of the model. 

I will be using random forest modeling, an extension of decision tree modeling. To learn more about this modeling technique, follow along in the [walkthrough](https://datascienceineducation.com/c14.html).

## Loading Packages
```{r, echo = FALSE}
library(tidyverse)
library(caret)
library(ranger)
library(e1071)
library(tidylog)
library(dataedu)
```

## Importing and Viewing the data
```{r, echo = FALSE}
df <- dataedu::sci_mo_with_text
```

```{r}
glimpse(df)
```

After using `glimpse(df)`, I notice that there are 74 variables and 606 observations. A lot of the variables come from discussion posts that the students created. Therefore, it is unnecessary to have all of these variables in our model. I will only select a few variables that I believe belong in our model to predict a final grade. 

I am only interested in using data from one specific, so I will need to narrow down the data. 

```{r}
df <- 
  df %>%
  select(
    int,
    uv,
    pc,
    time_spent,
    final_grade,
    subject,
    enrollment_reason,
    semester,
    enrollment_status,
    cogproc,
    social,
    posemo,
    negemo,
    n
  )
```

## Analysis
Next, I will remove observations with missing data.
```{r}
#checking how many rows are in the dataset
nrow(df)
```

```{r}
df <- na.omit(df)
```

```{r}
nrow(df)
```

After using `na.omit(df)`, I noticed that our number of rows decreased, which indicates that `na.omit` worked.

Some of the variables in the dataset will not be suitable for machine learning. The variables could be highly correlated with other variables, or there is no variability. For example, one of the variables in the current dataset has the same value for all of the observations. I will use a function to detect this variable and any others.

```{r}
nearZeroVar(df, saveMetrics = TRUE)
```

This function checks for zero variance, so I want to check the `zeroVar` column to see whether any variables failed this check. When a variable fails this check, the column will have an output of `TRUE`, which we see for `enrollment_status`. I see that it is "Approved/Enrolled" for all students when looking at this variable. Therefore, I will remove this variable because having no variability may cause problems for specific models. 

```{r}
df <-
  df %>%
  select(-enrollment_status)
```

Next, I will pre-process variables, which may be done by centering or scaling them. Another thing I want to pay attention to is the text data. We want the text data to be in a format to evaluate. Therefore, I will change the character strings into factors.

```{r}
df <-
  df %>%
  mutate_if(is.character, as.factor)
```

Next, I will prepare the train and test datasets. But, first, I will "set the seed," which ensures that I will get the same results in the data partition if I rerun this same code. 

```{r}
#setting seed
set.seed(2022)

#creating a new object called trainIndex that will take 80 percent of the data
trainIndex <- createDataPartition(df$final_grade,
                                  p = .8,
                                  list = FALSE,
                                  times = 1)

#adding a new temporary variable to the dataset
#this variable will allow me to select rows according to the row number
df <-
  df %>%
  mutate(temp_id = 1:464)


```
```{r}
#filtering the dataset so I only get the rows indicated by the trainIndex vector
df_train <- 
  df %>%
  filter(temp_id %in% trainIndex)
```

```{r}
#filtering dataset in a different way so that we get only the rows
#NOT in the trainIndex vector
df_test <-
  df %>%
  filter(!temp_id %in% trainIndex)
```

```{r}
#deleting the temporary variable
df <-
  df %>%
  select(-temp_id)
```

```{r}
df_train <- 
  df_train %>%
  select(-temp_id)
```

```{r}
df_test <-
  df_test %>%
  select(-temp_id)
```

Next, I will estimate the models. I will use the train function, passing all variables in the data frame as predictors, except for the outcome variable. 

There are three indicators of motivation that the virtual school measured, interest in the course (`int`), the perceived utility value of the course (`uv`), and perceived competence for the subject matter (`pc`). In addition, a couple of variables differentiate between the different courses in the dataset, the subject matter of the course (`subject`), the reason the student enrolled in the course (`enrollment_reason`), and the semester in which the course took place (`semester`). Lastly, the amount of time the student spent engaging with the online environment was measured and included as a predictor variable (`time_spent`).

The following variables are associated with the discussion board posts from the course:
- `cogproc` - the average level of cognitive processing in the discussion board posts
- `social` - the average level of social (rather than academic) content in the discussion board posts
- `posemo` and `negemo` - the positive and negative emotions evident in the discussion board posts
- `n` - the number of discussion board posts in total

I will set the seed again to ensure that our analysis is reproducible.  

```{r}
set.seed(2022)

#running the model
rf_fit <- train(final_grade ~.,
                data = df_train,
                method = "ranger")

#summary of the model
rf_fit
```

First, I see 372 samples, the number in the train data set. I did not specify any pre-processing steps in the model fitting. But, `preProcess` can be passed to `train()` to center, scale, and transform the data in many other ways. I used a resampling technique, which I used for selecting the tuning parameters. I can manually provide or estimate these parameters via bootstrap resampling or *k*-folds cross-validation strategies. 

To interpret the findings, I want to look to minimize the Root Mean Square Error (RMSE) and maximize the variance explained (rsquared).

Therefore, the model with the value of the `mtry` tuning parameter equal to 19 seemed to explain the data best with the `splitrule` being "extratrees", and `min.node.size` held constant at a value of 5. I know this model fits best because the RMSE is the lowest (13.87), and the Rsquared is the highest of the options (.59).

The variance would be higher without resampling, and the model's predictive accuracy would be lower. 

Next, I will use cross-validation as the resampling technique instead of bootstrapping to see whether I have different values. 
```{r}
set.seed(2022)

train_control <- 
  trainControl(method = "repeatedcv",
               number = 10,
               repeats = 10)

rf_fit1 <- 
  train(final_grade ~.,
                 data = df_train,
                 method = "ranger",
                 trControl = train_control)

rf_fit1
```

When looking at the output, I am looking for which values of the various tuning parameters were selected. For example, in the `mtry` column, the value was 19, the split rule is "extratrees," and the minimum node size is 5. The model explored which value of `mtry` was best and whether extra tree or variance was a better split rule. Still, I forced all model iterations to a minimum node size of five. 

Next, I will create my grid of values to test for `mtry` and `min.node.size`. I will stick with the default bootstrap resampling method to choose the best model. I'll randomly select some values for `mtry`, including the three used previously (2, 10, and 19). The values I'll try will be 2, 3, 7, 10, and 19. 
```{r}
#setting the seed
set.seed(2022)

#creating the grid of different values of mtry, different splitrules, and different min.node sizes to try
tune_grid <-
  expand.grid(
    mtry = c(2, 3, 7, 10, 19),
    splitrule = c("variance", "extratrees"),
    min.node.size = c(1, 5, 10, 15, 20)
  )

#fitting the new model using the tuning grid I created above
rf_fit2 <-
  train(final_grade ~.,
        data = df_train,
        method = "ranger",
        tuneGrid = tune_grid)

rf_fit2
```

The model with the same values as identified before for `mtry` (19) and `splitrule` (extratrees). Again the `min.node.size` is equal to 5 seems to fit best. 

Next, I will look at the model more closely. 

```{r}
rf_fit2$finalModel
```

From this output, I see that `mtry` is equal to 19, the node size is 5, and the split rule is extra trees. Also, the OOB prediction errror (MSE) is 173.86 and the proportion of the variance explained is 0.61. 

Next, I will use the test data set to be put through the model, and assign the predicted values to a column called `pred`. Also, I will create a `obs` column that includes the real final grades that students earned. Later, I will compare the predicted and observed values to see how well the model did. 

```{r}
set.seed(2022)

#creating a new object for the testing data
df_test_augmented <-
  df_test %>%
  mutate(pred = predict(rf_fit2, df_test),
         obs = final_grade)
```

```{r}
# transforming the new object into a data frame
defaultSummary(as.data.frame(df_test_augmented))
```

I can compare the above values to see how my model performs when given data not used to train the model. Comparing the RMSE values, I see that the RMSE for the train data was 13.82 and for the test data is 11.47, which is a slight improvement. In addition, the RSquared also improved a little from 0.59 to 0.75. Therefore, the model does marginally better with the test data compared to training data. 

## Results
For random forest models, we can learn which variables contributed most strongly to the prediction in the model across all the trees in our forest.

I will rerun the `rf_fit2` model with the exact specifications, but I will add an argument to call the variable importance metric.

```{r}
set.seed(2022)

rf_fit2_imp <-
  train(
    final_grade ~., 
    data = df_train,
    method = "ranger",
    tuneGrid = tune_grid,
    importance = "permutation"
  )

varImp(rf_fit2_imp)
```

Next, I am going to visualize the results from the above code. 

```{r}
varImp(rf_fit2_imp) %>%
  pluck(1) %>%
  rownames_to_column("var") %>%
  ggplot(aes(x = reorder(var, Overall), y = Overall)) +
  geom_col(fill = dataedu_colors("darkblue")) +
  coord_flip() +
  theme_dataedu()
```

The first thing I notice is that the variable `n` is the most important. This variable is related to the discussion posts students write and how much they write in their discussion posts. The second most important is `subjectFrSca`. Forensic science is the most crucial course, and being enrolled in this course impacts the final grade. The third most important variable is the time spent in their online course. 

There are subject differences in the final grade prediction for the psychology, biology, and forensic science course. In addition, the course the student enrolls in seems to affect the final grade depending on the course. Therefore, perhaps grades should be normalized within each course. Would this still be an essential predictor if I did this? However, I am not going to be diving into that next. 

## Comparing Random Forest to Regression
Below, I will specify a linear model (regression) and check out how the linear model performs in predicting the real outcomes. 

```{r}
#making sure the variables stored as characters are converted to factors
df_train_lm <-
  df_train %>%
  mutate_if(is.character, as.factor)
```
```{r}
#creating linear regression model
lm_fit <-
  train(final_grade ~.,
        data = df_train_lm,
        method = "lm")

#append the predicted values to the training dataset for the linear model,
#so I can see both the predicted and the actual values
df_train_lm <-
  df_train %>%
  mutate(obs = final_grade,
         pred = predict(lm_fit, df_train_lm))
```
```{r}
df_train_randomfor <-
  df_train %>%
  mutate(pred = predict(rf_fit2, df_train),
         obs = final_grade)
```
```{r}
#linear model
defaultSummary(as.data.frame(df_train_lm))
```
```{r}
#random forest
defaultSummary(as.data.frame(df_train_randomfor))
```
The random forest model performs better than the regression. A possible future direction is to use a more sophisticated model approach like deep learning. 

## Conclusion
I have learned how to apply machine learning to an educational dataset from this walkthrough. I learned specifically to use the machine learning technique called random forest modeling. Also, I learned that this model performs better on the dataset than linear regression. From the modeling, I learned that when predicting the final grade, how much a student is writing in their discussion posts is a good predictor for their final grade. I can assume from this result if a student is writing a lot in their discussion posts. They are highly engaged in what they are learning, which influences their final grade. Therefore, the instructor might pay attention to students who do not write that much in their discussion posts to engage those students more in what they are learning.    


